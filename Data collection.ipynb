{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "startRampPath = 'data/BLM/start_ramp/'\n",
    "startAdjustPath = 'data/BLM/start_adjust/'\n",
    "startSqueezePath = 'data/BLM/start_squeeze/'\n",
    "flatTopPath = 'data/BLM/flat_top/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLossRow(fill):\n",
    "    \"\"\"\n",
    "    Extract from the fill the row with the maximum loss.\n",
    "    \n",
    "    Parameters\n",
    "        fill : DataFrame -- DataFrame loaded from the BLM data fill file, without timestamp column\n",
    "    Returns\n",
    "        DataFrame with row with the max BLM value in the provided data\n",
    "    \"\"\"\n",
    "    maxes = fill.max(axis=1)  # row index -> max for that row\n",
    "    maxOverallIdx = maxes.idxmax() # row index where max overall in fill can be found\n",
    "    \n",
    "    return fill.loc[maxOverallIdx] # row containing max overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFillFilenames(phasePath):\n",
    "    files = sorted(os.listdir(phasePath))\n",
    "    files = [file for file in files if file.endswith('.txt')] # keep only text files - filter out files such as desktop.ini\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMergedPhaseLosses(phasePath, maxFills):\n",
    "    \"\"\"\n",
    "        In each fill file in phasePath (up to maxFills) the \"max loss row\" is found (i.e. searching for the highest \n",
    "        BLM value registered and taking the row within which it occurs).  This row is appended to a DataFrame.  After this task\n",
    "        is completed, B1 BLM and B2 BLM columns of interest (symmetric BLMs) are filtered out.  A new DataFrame is created whose \n",
    "        columns are \"logical\" BLM names - each representing a pair of symmetric BLMs.  The rows are therefore hierachically \n",
    "        indexed by beam number (B1 or B2) and fill number.\n",
    "    Parameters\n",
    "        phasePath : string - path containing fill files for a particular phase\n",
    "        maxFills : int - maximum number of fills to process.  -1 to process all fill files.\n",
    "    Returns\n",
    "        DataFrame hierarchically indexed by fill number and beam number (B1 or B2); columns are logical BLM names.  Each row is\n",
    "        the max loss row taken from the corresponding fill file. eg row B1, 7207 and row B2, 7207 both originate from\n",
    "        the same row in fill 7207 - the max loss row.  It was split into two separate rows differing by beam index (B1 and B2)\n",
    "        so that symmetric B1 BLMs and B2 BLMs could be merged under the same logical BLM column names.  \n",
    "    \"\"\"\n",
    "    filenames = getFillFilenames(phasePath)\n",
    "    phaseLosses = pd.DataFrame() # in each fill the row with the highest loss is chosen and appended to this DataFrame\n",
    "    maxLossRows = []\n",
    "    prevFillNo = -1 # used to assert that filenames are in chronological order of fill number\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if i == maxFills:\n",
    "            break\n",
    "\n",
    "        # load fill file into a DataFrame\n",
    "        fill = pd.read_csv(phasePath + filename, sep=' ', header=None)\n",
    "        # Delete first column (contains timestamps)\n",
    "        fill = fill.drop(columns=0)\n",
    "        # Extract the fill number from the filename (used eventually as the key for the row extracted from this fill)\n",
    "        fillNo = re.search(r'\\d+', filename).group()\n",
    "        assert int(fillNo) > prevFillNo, \"Fills not loaded in chronological order!  Fill \" + fillNo + \" came after \" + str(prevFillNo)\n",
    "\n",
    "        # extract from the fill the row with the maximum loss\n",
    "        rowWithMaxLoss = getMaxLossRow(fill)\n",
    "        rowWithMaxLoss.name = fillNo\n",
    "        maxLossRows.append(rowWithMaxLoss)\n",
    "        prevFillNo = int(fillNo)\n",
    "        \n",
    "    phaseLosses = phaseLosses.append(maxLossRows)\n",
    "        \n",
    "    blmLabels = np.genfromtxt('data/blm_labels.txt', dtype='str')\n",
    "    beam1BlmLabels = np.genfromtxt('data/beam1_blm_labels.txt', dtype='str')\n",
    "    beam2BlmLabels = np.genfromtxt('data/beam2_blm_labels.txt', dtype='str')\n",
    "    # NB: Each BLM in beam1BlmLabels corresponds row by row to its symmetric BLM in  beam2BlmLabels\n",
    "    logicalBlmLabels = np.genfromtxt('data/logical_blm_labels.txt', dtype='str')\n",
    "    assert len(beam1BlmLabels) == len(beam2BlmLabels) == len(logicalBlmLabels)\n",
    "\n",
    "    phaseLosses.columns = blmLabels\n",
    "\n",
    "    # Separate beam 1 losses and beam 2 losses\n",
    "    beam1PhaseLosses = phaseLosses.filter(items=beam1BlmLabels); \n",
    "    beam2PhaseLosses = phaseLosses.filter(items=beam2BlmLabels);\n",
    "\n",
    "    # merge beam 1 losses and beam 2 losses under the same logical BLM labels\n",
    "    beam1PhaseLosses.columns = logicalBlmLabels \n",
    "    beam2PhaseLosses.columns = logicalBlmLabels\n",
    "    mergedPhaseLosses = pd.concat([beam1PhaseLosses, beam2PhaseLosses], keys=['B1', 'B2'])\n",
    "    \n",
    "    return mergedPhaseLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxFills = -1\n",
    "startRampLosses = getMergedPhaseLosses(startRampPath, maxFills)\n",
    "startAdjustLosses = getMergedPhaseLosses(startAdjustPath, maxFills)\n",
    "startSqueezeLosses = getMergedPhaseLosses(startSqueezePath, maxFills)\n",
    "flatTopLosses = getMergedPhaseLosses(flatTopPath, maxFills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrames\n",
    "\n",
    "startRampLosses.to_pickle(\"data/pickles/startRampLosses.pkl\")\n",
    "startAdjustLosses.to_pickle(\"data/pickles/startAdjustLosses.pkl\")\n",
    "startSqueezeLosses.to_pickle(\"data/pickles/startSqueezeLosses.pkl\")\n",
    "flatTopLosses.to_pickle(\"data/pickles/flatTopLosses.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
